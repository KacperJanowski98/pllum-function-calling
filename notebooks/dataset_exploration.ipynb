{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Function Calling Dataset\n",
    "\n",
    "This notebook demonstrates how to load and explore the Salesforce/xlam-function-calling-60k dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Use a safe import for tqdm that will work with or without ipywidgets\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    print(\"Using notebook tqdm with progress bar\")\n",
    "except ImportError:\n",
    "    print(\"tqdm.notebook failed to import. Using standard tqdm.\")\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "    except ImportError:\n",
    "        # Fallback to a simple function if tqdm is completely unavailable\n",
    "        def tqdm(iterable, *args, **kwargs):\n",
    "            print(\"Processing items...\")\n",
    "            return iterable\n",
    "\n",
    "from src.dataset import load_function_calling_dataset, parse_json_entry, get_tool_names, filter_by_tool\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "Now, let's load the function calling dataset. This requires authentication with Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_function_calling_dataset()\n",
    "\n",
    "print(f'Dataset contains {len(dataset[\"train\"])} examples')\n",
    "print(f'Dataset features: {dataset[\"train\"].features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring a Sample Entry\n",
    "\n",
    "Let's examine one entry to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample entry\n",
    "sample = dataset['train'][0]\n",
    "\n",
    "# Parse the JSON strings\n",
    "parsed_sample = parse_json_entry(sample)\n",
    "\n",
    "# Print the query\n",
    "print(f'\\nQuery:\\n{parsed_sample[\"query\"]}')\n",
    "\n",
    "# Print the tools\n",
    "print('\\nTools:')\n",
    "for tool in parsed_sample['tools']:\n",
    "    print(f'\\n{tool[\"name\"]}:\\n{tool[\"description\"]}')\n",
    "    print('Parameters:')\n",
    "    for param_name, param_details in tool['parameters'].items():\n",
    "        print(f'  - {param_name}: {param_details[\"type\"]} - {param_details[\"description\"]}')\n",
    "\n",
    "# Print the answers\n",
    "print('\\nAnswers:')\n",
    "for answer in parsed_sample['answers']:\n",
    "    print(f'\\n{answer[\"name\"]}')\n",
    "    print('Arguments:')\n",
    "    for arg_name, arg_value in answer['arguments'].items():\n",
    "        print(f'  - {arg_name}: {arg_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Dataset\n",
    "\n",
    "Now let's perform some analysis on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tool names in the dataset\n",
    "tool_names = get_tool_names(dataset)\n",
    "\n",
    "print(f'Dataset contains {len(tool_names)} unique tools')\n",
    "print('\\nSample of tool names:')\n",
    "for name in tool_names[:10]:  # Show first 10 tool names\n",
    "    print(f'- {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Usage Distribution\n",
    "\n",
    "Let's analyze how frequently different tools are used in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tool usage\n",
    "tool_counts = {}\n",
    "\n",
    "# Only analyze a portion of the dataset for performance\n",
    "sample_size = 1000\n",
    "print(f\"Analyzing {sample_size} entries from the dataset...\")\n",
    "\n",
    "# Use a simple loop if tqdm fails\n",
    "for i, entry in enumerate(dataset['train'][:sample_size]):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i}/{sample_size} entries\")\n",
    "        \n",
    "    parsed = parse_json_entry(entry)\n",
    "    tools = parsed.get('tools', [])\n",
    "    \n",
    "    if isinstance(tools, list):\n",
    "        for tool in tools:\n",
    "            if isinstance(tool, dict) and 'name' in tool:\n",
    "                name = tool['name']\n",
    "                tool_counts[name] = tool_counts.get(name, 0) + 1\n",
    "\n",
    "print(f\"Finished processing {sample_size} entries\")\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "tool_df = pd.DataFrame({\n",
    "    'tool': list(tool_counts.keys()),\n",
    "    'count': list(tool_counts.values())\n",
    "})\n",
    "\n",
    "# Sort by count\n",
    "tool_df = tool_df.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display top tools in table format\n",
    "print(\"\\nTop 20 most common tools:\")\n",
    "display(tool_df.head(20))\n",
    "\n",
    "# Plot the top 20 most common tools\n",
    "plt.figure(figsize=(14, 8))\n",
    "top_tools = tool_df.head(20)\n",
    "sns.barplot(x='count', y='tool', data=top_tools)\n",
    "plt.title('Top 20 Most Common Tools')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Tool Name')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Length Analysis\n",
    "\n",
    "Let's analyze the distribution of query lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate query lengths\n",
    "query_lengths = []\n",
    "\n",
    "# Only analyze a portion of the dataset for performance\n",
    "sample_size = 1000\n",
    "print(f\"Analyzing {sample_size} entries from the dataset...\")\n",
    "\n",
    "for i, entry in enumerate(dataset['train'][:sample_size]):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i}/{sample_size} entries\")\n",
    "        \n",
    "    parsed = parse_json_entry(entry)\n",
    "    query = parsed.get('query', '')\n",
    "    if isinstance(query, str):\n",
    "        query_lengths.append(len(query))\n",
    "\n",
    "print(f\"Finished processing {sample_size} entries\")\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(query_lengths, kde=True, bins=30)\n",
    "plt.title('Distribution of Query Lengths')\n",
    "plt.xlabel('Query Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "query_length_series = pd.Series(query_lengths)\n",
    "print('\\nQuery Length Statistics:')\n",
    "print(query_length_series.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Analysis\n",
    "\n",
    "Let's analyze the number of parameters per tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of parameters per tool\n",
    "param_counts = []\n",
    "tool_param_data = []\n",
    "\n",
    "# Only analyze a portion of the dataset for performance\n",
    "sample_size = 1000\n",
    "print(f\"Analyzing {sample_size} entries from the dataset...\")\n",
    "\n",
    "for i, entry in enumerate(dataset['train'][:sample_size]):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i}/{sample_size} entries\")\n",
    "        \n",
    "    parsed = parse_json_entry(entry)\n",
    "    tools = parsed.get('tools', [])\n",
    "    \n",
    "    if isinstance(tools, list):\n",
    "        for tool in tools:\n",
    "            if isinstance(tool, dict) and 'name' in tool and 'parameters' in tool:\n",
    "                name = tool['name']\n",
    "                params = tool['parameters']\n",
    "                param_count = len(params)\n",
    "                param_counts.append(param_count)\n",
    "                tool_param_data.append({\n",
    "                    'tool': name,\n",
    "                    'param_count': param_count\n",
    "                })\n",
    "\n",
    "print(f\"Finished processing {sample_size} entries\")\n",
    "\n",
    "# Create a DataFrame\n",
    "param_df = pd.DataFrame(tool_param_data)\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(param_counts, kde=True, bins=15)\n",
    "plt.title('Distribution of Parameter Counts per Tool')\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate average parameters per tool\n",
    "tool_avg_params = param_df.groupby('tool')['param_count'].mean().reset_index()\n",
    "tool_avg_params = tool_avg_params.sort_values('param_count', ascending=False)\n",
    "\n",
    "# Plot top 20 tools by average parameter count\n",
    "plt.figure(figsize=(14, 8))\n",
    "top_param_tools = tool_avg_params.head(20)\n",
    "sns.barplot(x='param_count', y='tool', data=top_param_tools)\n",
    "plt.title('Top 20 Tools by Average Parameter Count')\n",
    "plt.xlabel('Average Number of Parameters')\n",
    "plt.ylabel('Tool Name')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Specific Tools\n",
    "\n",
    "Let's look at examples of a specific tool in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have tool_df from earlier cells\n",
    "try:\n",
    "    # Choose a tool from the top tools we saw earlier\n",
    "    target_tool = tool_df.iloc[0]['tool']  # Most common tool\n",
    "except NameError:\n",
    "    # If tool_df is not available, choose a default tool\n",
    "    print(\"tool_df not available. Using a default tool.\")\n",
    "    if len(tool_names) > 0:\n",
    "        target_tool = tool_names[0]\n",
    "    else:\n",
    "        # If we don't have tool names, get from first sample\n",
    "        sample = dataset['train'][0]\n",
    "        parsed = parse_json_entry(sample)\n",
    "        tools = parsed.get('tools', [])\n",
    "        if len(tools) > 0 and isinstance(tools[0], dict) and 'name' in tools[0]:\n",
    "            target_tool = tools[0]['name']\n",
    "        else:\n",
    "            print(\"Could not find a valid tool to examine\")\n",
    "            target_tool = None\n",
    "\n",
    "if target_tool:\n",
    "    print(f'Looking at examples of tool: {target_tool}')\n",
    "    \n",
    "    # Filter entries using this tool\n",
    "    filtered_entries = filter_by_tool(dataset, target_tool)\n",
    "    print(f'Found {len(filtered_entries)} entries using this tool')\n",
    "    \n",
    "    if filtered_entries:\n",
    "        # Display a sample\n",
    "        sample_entry = filtered_entries[0]\n",
    "        \n",
    "        print(f'\\nSample Query:\\n{sample_entry[\"query\"]}')\n",
    "        \n",
    "        # Find the tool details\n",
    "        tool_details = None\n",
    "        for tool in sample_entry['tools']:\n",
    "            if tool['name'] == target_tool:\n",
    "                tool_details = tool\n",
    "                break\n",
    "        \n",
    "        if tool_details:\n",
    "            print(f'\\nTool Description:\\n{tool_details[\"description\"]}')\n",
    "            print('\\nParameters:')\n",
    "            for param_name, param_details in tool_details['parameters'].items():\n",
    "                print(f'  - {param_name}: {param_details[\"type\"]} - {param_details[\"description\"]}')\n",
    "        \n",
    "        # Find the corresponding answer\n",
    "        answer = None\n",
    "        for ans in sample_entry['answers']:\n",
    "            if ans['name'] == target_tool:\n",
    "                answer = ans\n",
    "                break\n",
    "        \n",
    "        if answer:\n",
    "            print('\\nArguments Used:')\n",
    "            for arg_name, arg_value in answer['arguments'].items():\n",
    "                print(f'  - {arg_name}: {arg_value}')\n",
    "    else:\n",
    "        print(\"No entries found using this tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the basic structure and characteristics of the Salesforce/xlam-function-calling-60k dataset. The dataset contains a diverse set of function calling examples with tools, parameters, and usage patterns.\n",
    "\n",
    "Further analysis could include:\n",
    "\n",
    "1. Categorizing tools by domain or functionality\n",
    "2. Analyzing parameter types and constraints\n",
    "3. Examining patterns in query formulation\n",
    "4. Building models to predict appropriate tools for queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 }
}
