{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Translated Dataset\n",
    "\n",
    "This notebook demonstrates how to create a dataset with a percentage of queries translated from English to Polish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.auth import login_to_huggingface\n",
    "from src.dataset import load_function_calling_dataset, parse_json_entry\n",
    "from src.translation_dataset import (\n",
    "    create_translated_dataset,\n",
    "    analyze_translated_dataset\n",
    ")\n",
    "\n",
    "# Visual settings\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Small Test Dataset\n",
    "\n",
    "First, let's create a small test dataset with a few translated queries to verify that everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the test dataset\n",
    "test_output_path = Path('../data/test_translated_dataset.json')\n",
    "\n",
    "# Create a small dataset with 10 samples, 50% translated\n",
    "test_dataset_path = create_translated_dataset(\n",
    "    output_path=test_output_path,\n",
    "    sample_size=10,\n",
    "    translation_percentage=0.5,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"Test dataset created at: {test_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Test Dataset\n",
    "\n",
    "Let's analyze the test dataset to verify the translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the test dataset\n",
    "with open(test_output_path, 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Display the queries from the first few samples\n",
    "print(\"Queries in the test dataset:\")\n",
    "for i, sample in enumerate(test_data):\n",
    "    print(f\"Sample {i+1}: {sample['query']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the language distribution\n",
    "test_summary = analyze_translated_dataset(test_output_path)\n",
    "print(\"Language distribution in the test dataset:\")\n",
    "test_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Full Translated Dataset\n",
    "\n",
    "Now, let's create the full dataset with 40% of queries translated to Polish. Depending on the size of the dataset, this may take some time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the full dataset\n",
    "full_output_path = Path('../data/xlam_function_calling_pl.json')\n",
    "\n",
    "# Option 1: Creating a full dataset with all samples (this will take a long time)\n",
    "# full_dataset_path = create_translated_dataset(\n",
    "#     output_path=full_output_path,\n",
    "#     translation_percentage=0.4,\n",
    "#     random_seed=42\n",
    "# )\n",
    "\n",
    "# Option 2: Creating a more manageable dataset (e.g., 1000 samples)\n",
    "sample_size = 1000  # Adjust based on your needs and resources\n",
    "full_dataset_path = create_translated_dataset(\n",
    "    output_path=full_output_path,\n",
    "    sample_size=sample_size,\n",
    "    translation_percentage=0.4,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"Full dataset created at: {full_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Full Dataset\n",
    "\n",
    "Now, let's analyze the full dataset to verify that approximately 40% of the queries have been translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the language distribution\n",
    "full_summary = analyze_translated_dataset(full_output_path)\n",
    "print(\"Language distribution in the full dataset:\")\n",
    "full_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the language distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Language', y='Percentage', data=full_summary)\n",
    "plt.title('Language Distribution in the Translated Dataset')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('Percentage of Queries')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/language_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Examples of Translated Queries\n",
    "\n",
    "Let's look at some examples of translated queries to verify the quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset\n",
    "with open(full_output_path, 'r', encoding='utf-8') as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "# Function to detect if a query is in Polish (simplified)\n",
    "def is_polish(text):\n",
    "    return any(c in text for c in 'ąćęłńóśźżĄĆĘŁŃÓŚŹŻ')\n",
    "\n",
    "# Get examples of English and Polish queries\n",
    "english_examples = []\n",
    "polish_examples = []\n",
    "\n",
    "for sample in full_data:\n",
    "    query = sample['query']\n",
    "    if len(english_examples) < 5 and not is_polish(query):\n",
    "        english_examples.append(query)\n",
    "    elif len(polish_examples) < 5 and is_polish(query):\n",
    "        polish_examples.append(query)\n",
    "        \n",
    "    if len(english_examples) >= 5 and len(polish_examples) >= 5:\n",
    "        break\n",
    "\n",
    "# Display examples\n",
    "print(\"Examples of English Queries:\")\n",
    "for i, query in enumerate(english_examples):\n",
    "    print(f\"{i+1}. {query}\\n\")\n",
    "\n",
    "print(\"\\nExamples of Polish Queries:\")\n",
    "for i, query in enumerate(polish_examples):\n",
    "    print(f\"{i+1}. {query}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Dataset Structure\n",
    "\n",
    "Let's verify that the dataset structure is maintained, with only the queries translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample with a Polish query\n",
    "polish_sample = None\n",
    "for sample in full_data:\n",
    "    if is_polish(sample['query']):\n",
    "        polish_sample = sample\n",
    "        break\n",
    "\n",
    "if polish_sample:\n",
    "    # Parse the sample to examine its structure\n",
    "    parsed_sample = parse_json_entry(polish_sample)\n",
    "    \n",
    "    print(\"Structure of a sample with a Polish query:\")\n",
    "    print(f\"Query: {parsed_sample['query']}\\n\")\n",
    "    \n",
    "    print(\"Tools:\")\n",
    "    for tool in parsed_sample['tools'][:2]:  # Show first two tools\n",
    "        print(f\"- Name: {tool['name']}\")\n",
    "        print(f\"  Description: {tool['description']}\")\n",
    "        print(f\"  Parameters: {list(tool['parameters'].keys())}\\n\")\n",
    "    \n",
    "    print(\"Answers:\")\n",
    "    for answer in parsed_sample['answers'][:2]:  # Show first two answers\n",
    "        print(f\"- Tool: {answer['name']}\")\n",
    "        print(f\"  Arguments: {answer['arguments']}\\n\")\n",
    "else:\n",
    "    print(\"No sample with a Polish query found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataset Statistics\n",
    "\n",
    "Finally, let's save some statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset statistics\n",
    "stats = {\n",
    "    \"total_samples\": len(full_data),\n",
    "    \"english_samples\": int(full_summary[full_summary['Language'] == 'English']['Count'].values[0] \n",
    "                           if 'English' in full_summary['Language'].values else 0),\n",
    "    \"polish_samples\": int(full_summary[full_summary['Language'] == 'Polish']['Count'].values[0] \n",
    "                          if 'Polish' in full_summary['Language'].values else 0),\n",
    "    \"polish_percentage\": float(full_summary[full_summary['Language'] == 'Polish']['Percentage'].values[0] \n",
    "                              if 'Polish' in full_summary['Language'].values else 0),\n",
    "}\n",
    "\n",
    "# Display statistics\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Save statistics to file\n",
    "stats_path = Path('../data/dataset_stats.json')\n",
    "with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(f\"\\nStatistics saved to {stats_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Created a small test dataset to verify the translation functionality\n",
    "2. Created a larger dataset with approximately 40% of queries translated to Polish\n",
    "3. Analyzed and visualized the language distribution in the dataset\n",
    "4. Verified that the dataset structure is maintained, with only the queries translated\n",
    "5. Saved statistics about the dataset\n",
    "\n",
    "The resulting dataset can now be used to fine-tune the PLLUM model for function-calling tasks in both English and Polish."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}